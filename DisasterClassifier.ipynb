{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from sklearn.metrics import f1_score\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Set device\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### SETTINGS\n",
    "##########################\n",
    "# Hyperparameters\n",
    "RANDOM_SEED = 1\n",
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 10\n",
    "GRAYSCALE = False\n",
    "DATA_DIR = \"./dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "46\n",
      "16\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "# Transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Custom Dataset class\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.image_folder_dataset = ImageFolder(root=self.data_dir, transform=self.transform)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_folder_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.image_folder_dataset[idx]\n",
    "\n",
    "    @property\n",
    "    def classes(self):\n",
    "        return self.image_folder_dataset.classes\n",
    "\n",
    "# Load dataset and split into train, validation, test\n",
    "custom_dataset = CustomDataset(DATA_DIR, transform=transform)\n",
    "train_size = int(0.6 * len(custom_dataset))\n",
    "val_size = int(0.2 * len(custom_dataset))\n",
    "test_size = len(custom_dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(custom_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print( len(custom_dataset.classes))\n",
    "print( len(train_dataloader))\n",
    "print( len(val_dataloader))\n",
    "print( len(test_dataloader))\n",
    "\n",
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, len(custom_dataset.classes))\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct_pred = 0\n",
    "    total_examples = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, targets in data_loader:\n",
    "            features = features.to(device)\n",
    "            targets = targets.to(device)\n",
    "            logits = model(features)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted_labels = torch.max(logits, 1)\n",
    "            correct_pred += (predicted_labels == targets).sum().item()\n",
    "            total_examples += targets.size(0)\n",
    "\n",
    "            all_predictions.extend(predicted_labels.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "    accuracy = correct_pred / total_examples * 100\n",
    "    average_loss = total_loss / total_examples\n",
    "    f1 = f1_score(all_targets, all_predictions, average='weighted')\n",
    "    \n",
    "    return accuracy, average_loss, f1\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "train_f1_scores = []\n",
    "\n",
    "valid_losses = []\n",
    "valid_accuracies = []\n",
    "valid_f1_scores = []\n",
    "\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "test_f1_scores = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    epoch_train_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (features, targets) in enumerate(train_dataloader):\n",
    "        features = features.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(features)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_train_loss += loss.item()\n",
    "        \n",
    "        if not batch_idx % 120:\n",
    "            print(f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n",
    "                  f'Batch {batch_idx:03d}/{len(train_dataloader):03d} |' \n",
    "                  f' Cost: {loss:.4f}')\n",
    "    \n",
    "    train_loss = epoch_train_loss / len(train_dataloader)\n",
    "    train_acc, _, train_f1 = compute_metrics(model, train_dataloader, DEVICE)\n",
    "    val_acc, val_loss, val_f1 = compute_metrics(model, val_dataloader, DEVICE)\n",
    "    test_acc, test_loss, test_f1 = compute_metrics(model, test_dataloader, DEVICE)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} '\n",
    "          f'Train Acc.: {train_acc:.2f}% | Train Loss: {train_loss:.4f} | Train F1: {train_f1:.4f} '\n",
    "          f'| Validation Acc.: {val_acc:.2f}% | Validation Loss: {val_loss:.4f} | Validation F1: {val_f1:.4f}'\n",
    "          f'| Test Acc.: {test_acc:.2f}% | Test Loss: {test_loss:.4f} | Test F1: {test_f1:.4f}')\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    train_f1_scores.append(train_f1)\n",
    "\n",
    "    valid_losses.append(val_loss)\n",
    "    valid_accuracies.append(val_acc)\n",
    "    valid_f1_scores.append(val_f1)\n",
    "\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_acc)\n",
    "    test_f1_scores.append(test_f1)\n",
    "\n",
    "    elapsed = (time.time() - start_time) / 60\n",
    "    print(f'Time elapsed: {elapsed:.2f} min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Summary\n",
    "print(\"Training Summary:\")\n",
    "print(\"Train Loss:\", train_losses[-1])\n",
    "print(\"Train Accuracy:\", train_accuracies[-1])\n",
    "print(\"Train F1 Score:\", train_f1_scores[-1])\n",
    "\n",
    "print(\"\\nValidation Summary:\")\n",
    "print(\"Validation Loss:\", valid_losses[-1])\n",
    "print(\"Validation Accuracy:\", valid_accuracies[-1])\n",
    "print(\"Validation F1 Score:\", valid_f1_scores[-1])\n",
    "\n",
    "print(\"\\nTest Summary:\")\n",
    "print(\"Test Loss:\", test_losses[-1])\n",
    "print(\"Test Accuracy:\", test_accuracies[-1])\n",
    "print(\"Test F1 Score:\", test_f1_scores[-1])\n",
    "\n",
    "# Plotting History\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(valid_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss History')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(valid_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy History')\n",
    "plt.legend\n",
    "\n",
    "# Saving History Summary\n",
    "history_summary = {\n",
    "    \"train_losses\": train_losses,\n",
    "    \"valid_losses\": valid_losses,\n",
    "    \"test_losses\": test_losses,\n",
    "    \"train_accuracies\": train_accuracies,\n",
    "    \"valid_accuracies\": valid_accuracies,\n",
    "    \"test_accuracies\": test_accuracies,\n",
    "    \"train_f1_scores\": train_f1_scores,\n",
    "    \"valid_f1_scores\": valid_f1_scores,\n",
    "    \"test_f1_scores\": test_f1_scores\n",
    "}\n",
    "\n",
    "# Save to a file\n",
    "import json\n",
    "\n",
    "with open(\"history_summary.json\", \"w\") as outfile:\n",
    "    json.dump(history_summary, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "# def compute_metrics(model, data_loader, device):\n",
    "#     model.eval()\n",
    "#     correct_pred = 0\n",
    "#     total_examples = 0\n",
    "#     all_predictions = []\n",
    "#     all_targets = []\n",
    "#     total_loss = 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for features, targets in data_loader:\n",
    "#             features = features.to(device)\n",
    "#             targets = targets.to(device)\n",
    "#             logits = model(features)\n",
    "#             loss = F.cross_entropy(logits, targets)\n",
    "#             total_loss += loss.item()\n",
    "\n",
    "#             _, predicted_labels = torch.max(logits, 1)\n",
    "#             correct_pred += (predicted_labels == targets).sum().item()\n",
    "#             total_examples += targets.size(0)\n",
    "\n",
    "#             all_predictions.extend(predicted_labels.cpu().numpy())\n",
    "#             all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "#     accuracy = correct_pred / total_examples * 100\n",
    "#     average_loss = total_loss / total_examples\n",
    "#     f1 = f1_score(all_targets, all_predictions, average='weighted')\n",
    "    \n",
    "#     return accuracy, average_loss, f1\n",
    "\n",
    "# start_time = time.time()\n",
    "\n",
    "# for epoch in range(NUM_EPOCHS):\n",
    "#     model.train()\n",
    "#     epoch_train_loss = 0.0\n",
    "    \n",
    "#     for batch_idx, (features, targets) in enumerate(train_dataloader):\n",
    "#         features = features.to(DEVICE)\n",
    "#         targets = targets.to(DEVICE)\n",
    "#         optimizer.zero_grad()\n",
    "#         logits = model(features)\n",
    "#         loss = F.cross_entropy(logits, targets)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         epoch_train_loss += loss.item()\n",
    "        \n",
    "#         if not batch_idx % 120:\n",
    "#             print(f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n",
    "#                   f'Batch {batch_idx:03d}/{len(train_dataloader):03d} |' \n",
    "#                   f' Cost: {loss:.4f}')\n",
    "    \n",
    "#     train_loss = epoch_train_loss / len(train_dataloader)\n",
    "#     train_acc, _, train_f1 = compute_metrics(model, train_dataloader, DEVICE)\n",
    "#     val_acc, val_loss, val_f1 = compute_metrics(model, val_dataloader, DEVICE)\n",
    "#     test_acc, test_loss, test_f1 = compute_metrics(model, test_dataloader, DEVICE)\n",
    "\n",
    "#     print(f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} '\n",
    "#           f'Train Acc.: {train_acc:.2f}% | Train Loss: {train_loss:.4f} | Train F1: {train_f1:.4f} '\n",
    "#           f'| Validation Acc.: {val_acc:.2f}% | Validation Loss: {val_loss:.4f} | Validation F1: {val_f1:.4f}'\n",
    "#           f'| Test Acc.: {test_acc:.2f}% | Test Loss: {test_loss:.4f} | Test F1: {test_f1:.4f}')\n",
    "\n",
    "#     elapsed = (time.time() - start_time) / 60\n",
    "#     print(f'Time elapsed: {elapsed:.2f} min')\n",
    "\n",
    "# elapsed = (time.time() - start_time) / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def conv3x3(in_planes, out_planes, stride=1):\n",
    "#     \"\"\"3x3 convolution with padding\"\"\"\n",
    "#     return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "#                      padding=1, bias=False)\n",
    "\n",
    "\n",
    "# class Bottleneck(nn.Module):\n",
    "#     expansion = 4\n",
    "\n",
    "#     def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "#         super(Bottleneck, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "#         self.bn1 = nn.BatchNorm2d(planes)\n",
    "#         self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "#                                padding=1, bias=False)\n",
    "#         self.bn2 = nn.BatchNorm2d(planes)\n",
    "#         self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "#         self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "#         self.relu = nn.ReLU(inplace=True)\n",
    "#         self.downsample = downsample\n",
    "#         self.stride = stride\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         residual = x\n",
    "\n",
    "#         out = self.conv1(x)\n",
    "#         out = self.bn1(out)\n",
    "#         out = self.relu(out)\n",
    "\n",
    "#         out = self.conv2(out)\n",
    "#         out = self.bn2(out)\n",
    "#         out = self.relu(out)\n",
    "\n",
    "#         out = self.conv3(out)\n",
    "#         out = self.bn3(out)\n",
    "\n",
    "#         if self.downsample is not None:\n",
    "#             residual = self.downsample(x)\n",
    "\n",
    "#         out += residual\n",
    "#         out = self.relu(out)\n",
    "\n",
    "#         return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# class ResNet(nn.Module):\n",
    "\n",
    "#     def __init__(self, block, layers, num_classes, grayscale):\n",
    "#         self.inplanes = 64\n",
    "#         if grayscale:\n",
    "#             in_dim = 1\n",
    "#         else:\n",
    "#             in_dim = 3\n",
    "#         super(ResNet, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_dim, 64, kernel_size=7, stride=2, padding=3,\n",
    "#                                bias=False)\n",
    "#         self.bn1 = nn.BatchNorm2d(64)\n",
    "#         self.relu = nn.ReLU(inplace=True)\n",
    "#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "#         self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "#         self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "#         self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "#         self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "#         self.avgpool = nn.AvgPool2d(7, stride=1, padding=2)\n",
    "#         #self.fc = nn.Linear(2048 * block.expansion, num_classes)\n",
    "#         self.fc = nn.Linear(2048, num_classes)\n",
    "\n",
    "#         for m in self.modules():\n",
    "#             if isinstance(m, nn.Conv2d):\n",
    "#                 n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "#                 m.weight.data.normal_(0, (2. / n)**.5)\n",
    "#             elif isinstance(m, nn.BatchNorm2d):\n",
    "#                 m.weight.data.fill_(1)\n",
    "#                 m.bias.data.zero_()\n",
    "\n",
    "#     def _make_layer(self, block, planes, blocks, stride=1):\n",
    "#         downsample = None\n",
    "#         if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "#             downsample = nn.Sequential(\n",
    "#                 nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "#                           kernel_size=1, stride=stride, bias=False),\n",
    "#                 nn.BatchNorm2d(planes * block.expansion),\n",
    "#             )\n",
    "\n",
    "#         layers = []\n",
    "#         layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "#         self.inplanes = planes * block.expansion\n",
    "#         for i in range(1, blocks):\n",
    "#             layers.append(block(self.inplanes, planes))\n",
    "\n",
    "#         return nn.Sequential(*layers)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.bn1(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.maxpool(x)\n",
    "\n",
    "#         x = self.layer1(x)\n",
    "#         x = self.layer2(x)\n",
    "#         x = self.layer3(x)\n",
    "#         x = self.layer4(x)\n",
    "\n",
    "#         #x = self.avgpool(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         logits = self.fc(x)\n",
    "#         probas = F.softmax(logits, dim=1)\n",
    "#         return logits, probas\n",
    "\n",
    "\n",
    "\n",
    "# def resnet101(num_classes, grayscale):\n",
    "#     \"\"\"Constructs a ResNet-101 model.\"\"\"\n",
    "#     model = ResNet(block=Bottleneck, \n",
    "#                    layers=[3, 4, 23, 3],\n",
    "#                    num_classes=NUM_CLASSES,\n",
    "#                    grayscale=grayscale)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
