{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a93f8b4-deff-4337-86d3-dd2723e43007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import shutil\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from PIL import Image, ImageFile\n",
    "from torch.utils.data import random_split, Dataset\n",
    "from torchvision import transforms as T\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchmetrics\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9c60240-fa43-44d2-91dd-866d400bd768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n",
      "31\n",
      "976\n",
      "{'dataset\\\\drought': 0, 'dataset\\\\earthquake': 1, 'dataset\\\\human_damage': 2, 'dataset\\\\infrastructure': 3, 'dataset\\\\land_slide': 4, 'dataset\\\\non_damage_buildings_street': 5, 'dataset\\\\non_damage_human': 6, 'dataset\\\\non_damage_sea': 7, 'dataset\\\\non_damage_wildlife_forest': 8, 'dataset\\\\urban_fire': 9, 'dataset\\\\water_disaster': 10, 'dataset\\\\wild_fire': 11}\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(2024)\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root, transformations=None):\n",
    "        self.transformations = transformations\n",
    "        self.im_paths = sorted(glob(f\"{root}/*/*.png\"))\n",
    "\n",
    "        self.cls_names, self.cls_counts, count, data_count = {}, {}, 0, 0\n",
    "        for idx, im_path in enumerate(self.im_paths):\n",
    "            class_name = self.get_class(im_path)\n",
    "            if class_name not in self.cls_names:\n",
    "                self.cls_names[class_name] = count\n",
    "                self.cls_counts[class_name] = 1\n",
    "                count += 1\n",
    "            else:\n",
    "                self.cls_counts[class_name] += 1\n",
    "\n",
    "    def get_class(self, path):\n",
    "        return os.path.dirname(path).split(\"/\")[-1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.im_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        im_path = self.im_paths[idx]\n",
    "        im = Image.open(im_path).convert(\"RGB\")\n",
    "        gt = self.cls_names[self.get_class(im_path)]\n",
    "\n",
    "        if self.transformations is not None:\n",
    "            im = self.transformations(im)\n",
    "\n",
    "        return im, gt\n",
    "\n",
    "def get_dls(root, transformations, bs, split=[0.6, 0.2, 0.2], ns=4):\n",
    "    ds = CustomDataset(root=root, transformations=transformations)\n",
    "\n",
    "    total_len = len(ds)\n",
    "    tr_len = int(total_len * split[0])\n",
    "    vl_len = int(total_len * split[1])\n",
    "    ts_len = total_len - (tr_len + vl_len)\n",
    "\n",
    "    tr_ds, vl_ds, ts_ds = random_split(dataset=ds, lengths=[tr_len, vl_len, ts_len])\n",
    "\n",
    "    tr_dl = DataLoader(tr_ds, batch_size=bs, shuffle=True, num_workers=ns)\n",
    "    val_dl = DataLoader(vl_ds, batch_size=bs, shuffle=False, num_workers=ns)\n",
    "    ts_dl = DataLoader(ts_ds, batch_size=1, shuffle=False, num_workers=ns)\n",
    "\n",
    "    return tr_dl, val_dl, ts_dl, ds.cls_names\n",
    "\n",
    "root = \"./dataset\"\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "im_size = 224\n",
    "tfs = T.Compose([\n",
    "    T.Resize((im_size, im_size)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=mean, std=std)\n",
    "])\n",
    "tr_dl, val_dl, ts_dl, classes = get_dls(root=root, transformations=tfs, bs=32)\n",
    "\n",
    "print(len(tr_dl))\n",
    "print(len(val_dl))\n",
    "print(len(ts_dl))\n",
    "print(classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3bf834-cf3f-4015-be0b-7f2c999a5657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_2_im(t, t_type = \"rgb\"):\n",
    "    \n",
    "    gray_tfs = T.Compose([T.Normalize(mean = [ 0.], std = [1/0.5]), T.Normalize(mean = [-0.5], std = [1])])\n",
    "    rgb_tfs = T.Compose([T.Normalize(mean = [ 0., 0., 0. ], std = [ 1/0.229, 1/0.224, 1/0.225 ]), T.Normalize(mean = [ -0.485, -0.456, -0.406 ], std = [ 1., 1., 1. ])])\n",
    "    \n",
    "    invTrans = gray_tfs if t_type == \"gray\" else rgb_tfs \n",
    "    \n",
    "    return (invTrans(t) * 255).detach().squeeze().cpu().permute(1,2,0).numpy().astype(np.uint8) if t_type == \"gray\" else (invTrans(t) * 255).detach().cpu().permute(1,2,0).numpy().astype(np.uint8)\n",
    "\n",
    "def visualize(data, n_ims, rows, cmap = None, cls_names = None):\n",
    "    \n",
    "    assert cmap in [\"rgb\", \"gray\"], \"Specify if the image should be grayscale or colored!\"\n",
    "    if cmap == \"rgb\": cmap = \"viridis\"\n",
    "    \n",
    "    plt.figure(figsize = (20, 10))\n",
    "    indices = [random.randint(0, len(data) - 1) for _ in range(n_ims)]\n",
    "    for idx, indeks in enumerate(indices):\n",
    "        \n",
    "        im, gt = data[indeks]\n",
    "        # Start plot\n",
    "        plt.subplot(rows, n_ims // rows, idx + 1)\n",
    "        if cmap: plt.imshow(tensor_2_im(im, cmap), cmap=cmap)\n",
    "        else: plt.imshow(tensor_2_im(im))\n",
    "        plt.axis('off')\n",
    "        if cls_names is not None: plt.title(f\"GT -> {cls_names[int(gt)]}\")\n",
    "        else: plt.title(f\"GT -> {gt}\")\n",
    "            \n",
    "visualize(tr_dl.dataset, 20, 4, \"rgb\", list(classes.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bab885-7f79-4cc5-a72d-66961d8c8358",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(val_dl.dataset, 20, 4, \"rgb\", list(classes.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aad5523-36cc-4333-8967-8a081e23cd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(ts_dl.dataset, 20, 4, \"rgb\", list(classes.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3757d13b-9860-48a0-8032-728cdb0785fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_analysis(root, transformations):\n",
    "    \n",
    "    ds = CustomDataset(root = root, transformations = transformations)\n",
    "    cls_counts, width, text_width = ds.cls_counts,  0.7, 0.05\n",
    "    text_height = 2\n",
    "    cls_names = list(cls_counts.keys()); counts = list(cls_counts.values())\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize = (20, 10))\n",
    "    indices = np.arange(len(counts))\n",
    "\n",
    "    ax.bar(indices, counts, width, color = \"firebrick\")\n",
    "    ax.set_xlabel(\"Class Names\", color = \"red\")\n",
    "    ax.set_xticklabels(cls_names, rotation = 60)\n",
    "    ax.set(xticks = indices, xticklabels = cls_names)\n",
    "    ax.set_ylabel(\"Data Counts\", color = \"red\")\n",
    "    ax.set_title(f\"Dataset Class Imbalance Analysis\")\n",
    "\n",
    "    for i, v in enumerate(counts): ax.text(i - text_width, v + text_height, str(v), color = \"royalblue\")\n",
    "    \n",
    "data_analysis(root = root, transformations = tfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2656e66b-0b7d-4dce-86df-4f759eeea8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = timm.create_model('resnet50', pretrained=True)\n",
    "\n",
    "class CustomResNet(nn.Module):\n",
    "    def __init__(self, backbone, num_classes):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        in_features = backbone.fc.in_features\n",
    "        self.fc = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "m = CustomResNet(m, len(classes))\n",
    "\n",
    "def train_setup(m):\n",
    "    return m.to(device).eval(), 10, device, nn.CrossEntropyLoss(), optim.Adam(params=m.parameters(), lr=3e-4)\n",
    "\n",
    "def to_device(batch, device):\n",
    "    return batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "def get_metrics(model, ims, gts, loss_fn, epoch_loss, epoch_acc, epoch_f1):\n",
    "    preds = model(ims)\n",
    "    loss = loss_fn(preds, gts)\n",
    "    return loss, epoch_loss + loss.item(), epoch_acc + (torch.argmax(preds, dim=1) == gts).sum().item(), epoch_f1 + f1_score(preds.cpu().detach().numpy(), gts.cpu().detach().numpy(), average='macro')\n",
    "\n",
    "m, epochs, device, loss_fn, optimizer = train_setup(m)\n",
    "\n",
    "f1_score = torchmetrics.F1Score(num_classes=len(classes), average='macro',task='multiclass').to(device)\n",
    "\n",
    "save_prefix, save_dir = \"disaster\", \"saved_models\"\n",
    "print(\"Start training...\")\n",
    "\n",
    "best_loss, threshold, not_improved, patience = float(\"inf\"), 0.01, 0, 5\n",
    "tr_losses, val_losses, tr_accs, val_accs, tr_f1s, val_f1s = [], [], [], [], [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss, epoch_acc, epoch_f1 = 0, 0, 0\n",
    "\n",
    "    for idx, batch in tqdm(enumerate(tr_dl)):\n",
    "        ims, gts = to_device(batch, device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss, epoch_loss, epoch_acc, epoch_f1 = get_metrics(m, ims, gts, loss_fn, epoch_loss, epoch_acc, epoch_f1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    tr_loss_to_track = epoch_loss / len(tr_dl)\n",
    "    tr_acc_to_track = epoch_acc / len(tr_dl.dataset)\n",
    "    tr_f1_to_track = epoch_f1 / len(tr_dl)\n",
    "    tr_losses.append(tr_loss_to_track)\n",
    "    tr_accs.append(tr_acc_to_track)\n",
    "    tr_f1s.append(tr_f1_to_track)\n",
    "\n",
    "    print(f\"{epoch + 1}-epoch train process is completed!\")\n",
    "    print(f\"{epoch + 1}-epoch train loss          -> {tr_loss_to_track:.3f}\")\n",
    "    print(f\"{epoch + 1}-epoch train accuracy      -> {tr_acc_to_track:.3f}\")\n",
    "    print(f\"{epoch + 1}-epoch train f1-score      -> {tr_f1_to_track:.3f}\")\n",
    "\n",
    "    m.eval()\n",
    "    with torch.no_grad():\n",
    "        val_epoch_loss, val_epoch_acc, val_epoch_f1 = 0, 0, 0\n",
    "        for idx, batch in enumerate(val_dl):\n",
    "            ims, gts = to_device(batch, device)\n",
    "            loss, val_epoch_loss, val_epoch_acc, val_epoch_f1 = get_metrics(m, ims, gts, loss_fn, val_epoch_loss, val_epoch_acc, val_epoch_f1)\n",
    "\n",
    "        val_loss_to_track = val_epoch_loss / len(val_dl)\n",
    "        val_acc_to_track = val_epoch_acc / len(val_dl.dataset)\n",
    "        val_f1_to_track = val_epoch_f1 / len(val_dl)\n",
    "        val_losses.append(val_loss_to_track)\n",
    "        val_accs.append(val_acc_to_track)\n",
    "        val_f1s.append(val_f1_to_track)\n",
    "\n",
    "        print(f\"{epoch + 1}-epoch validation process is completed!\")\n",
    "        print(f\"{epoch + 1}-epoch validation loss     -> {val_loss_to_track:.3f}\")\n",
    "        print(f\"{epoch + 1}-epoch validation accuracy -> {val_acc_to_track:.3f}\")\n",
    "        print(f\"{epoch + 1}-epoch validation f1-score -> {val_f1_to_track:.3f}\")\n",
    "\n",
    "        if val_loss_to_track < (best_loss + threshold):\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            best_loss = val_loss_to_track\n",
    "            torch.save(m.state_dict(), f\"{save_dir}/{save_prefix}_best_model.pth\")\n",
    "        else:\n",
    "            not_improved += 1\n",
    "            print(f\"Loss value did not decrease for {not_improved} epochs\")\n",
    "            if not_improved == patience:\n",
    "                print(f\"Stop training since loss value did not decrease for {patience} epochs.\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d44097f-5e1f-4c86-99ce-886bd39fa2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlotLearningCurves:\n",
    "    \n",
    "    def __init__(self, tr_losses, val_losses, tr_accs, val_accs, tr_f1s, val_f1s):\n",
    "        \n",
    "        self.tr_losses, self.val_losses, self.tr_accs, self.val_accs, self.tr_f1s, self.val_f1s = tr_losses, val_losses, tr_accs, val_accs, tr_f1s, val_f1s\n",
    "        \n",
    "    def plot(self, array_1, array_2, label_1, label_2, color_1, color_2):\n",
    "        \n",
    "        plt.plot(array_1, label = label_1, c = color_1); plt.plot(array_2, label = label_2, c = color_2)\n",
    "        \n",
    "    def create_figure(self): plt.figure(figsize = (10, 5))\n",
    "    \n",
    "    def decorate(self, ylabel, xlabel = \"Epochs\"): \n",
    "        \n",
    "        plt.xlabel(xlabel); plt.ylabel(ylabel)\n",
    "        plt.xticks(ticks = np.arange(len(self.tr_accs)), labels = [i for i in range(1, len(self.tr_accs) + 1)])\n",
    "        plt.legend(); plt.show()      \n",
    "        \n",
    "    def visualize(self):\n",
    "        \n",
    "        # Figure 1\n",
    "        self.create_figure()\n",
    "        self.plot(array_1 = self.tr_losses, array_2 = self.val_losses, label_1 = \"Train Loss\", label_2 = \"Validation Loss\", color_1 = \"red\", color_2 = \"blue\"); self.decorate(ylabel = \"Loss Values\")\n",
    "        \n",
    "        # Figure 2\n",
    "        self.create_figure()\n",
    "        self.plot(array_1 = self.tr_accs, array_2 = self.val_accs, label_1 = \"Train Accuracy\", label_2 = \"Validation Accuracy\", color_1 = \"orangered\", color_2 = \"darkgreen\")\n",
    "        self.decorate(ylabel = \"Accuracy Scores\")\n",
    "        \n",
    "        # Figure 3\n",
    "        self.create_figure()\n",
    "        self.plot(array_1 = [tr_f1.cpu() for tr_f1 in self.tr_f1s], array_2 = [vl_f1.cpu() for vl_f1 in self.val_f1s], label_1 = \"Train F1 Score\", label_2 = \"Validation F1 Score\", color_1 = \"blueviolet\", color_2 = \"crimson\"); self.decorate(ylabel = \"F1 Scores\")\n",
    "        \n",
    "PlotLearningCurves(tr_losses, val_losses, tr_accs, val_accs, tr_f1s, val_f1s).visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5e51f0-afd5-42ca-bc02-dbd73bb40ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "class SaveFeatures():\n",
    "    \n",
    "    \"\"\" Extract pretrained activations\"\"\"\n",
    "    features = None\n",
    "    def __init__(self, m):\n",
    "        self.hook = m.register_forward_hook(self.hook_fn)\n",
    "    def hook_fn(self, module, input, output):\n",
    "        self.features = ((output.cpu()).data).numpy()\n",
    "    def remove(self): self.hook.remove()\n",
    "\n",
    "def getCAM(conv_fs, linear_weights, class_idx):\n",
    "    \n",
    "    bs, chs, h, w = conv_fs.shape\n",
    "    cam = linear_weights[class_idx].dot(conv_fs[0,:, :, ].reshape((chs, h * w)))\n",
    "    cam = cam.reshape(h, w)\n",
    "    \n",
    "    return (cam - np.min(cam)) / np.max(cam)\n",
    "\n",
    "def inference(model, device, test_dl, num_ims, row, final_conv, fc_params, cls_names = None):\n",
    "    \n",
    "    weight, acc = np.squeeze(fc_params[0].cpu().data.numpy()), 0\n",
    "    activated_features = SaveFeatures(final_conv)\n",
    "    preds, images, lbls = [], [], []\n",
    "    for idx, batch in tqdm(enumerate(test_dl)):\n",
    "        im, gt = to_device(batch, device)\n",
    "        pred_class = torch.argmax(model(im), dim = 1)\n",
    "        acc += (pred_class == gt).sum().item()\n",
    "        images.append(im)\n",
    "        preds.append(pred_class.item())\n",
    "        lbls.append(gt.item())\n",
    "    \n",
    "    print(f\"Accuracy of the model on the test data -> {(acc / len(test_dl.dataset)):.3f}\")\n",
    "    \n",
    "    plt.figure(figsize = (20, 10))\n",
    "    indekslar = [random.randint(0, len(images) - 1) for _ in range(num_ims)]\n",
    "    \n",
    "    for idx, indeks in enumerate(indekslar):\n",
    "        \n",
    "        im = images[indeks].squeeze()\n",
    "        pred_idx = preds[indeks]\n",
    "        heatmap = getCAM(activated_features.features, weight, pred_idx)\n",
    "        \n",
    "        # Start plot\n",
    "        plt.subplot(row, num_ims // row, idx + 1)\n",
    "        plt.imshow(tensor_2_im(im), cmap = \"gray\"); plt.axis(\"off\")\n",
    "        plt.imshow(cv2.resize(heatmap, (im_size, im_size), interpolation=cv2.INTER_LINEAR), alpha=0.4, cmap='jet'); plt.axis(\"off\")\n",
    "        \n",
    "        if cls_names is not None: plt.title(f\"GT -> {cls_names[int(lbls[indeks])]} ; PRED -> {cls_names[int(preds[indeks])]}\", color=(\"green\" if {cls_names[int(lbls[indeks])]} == {cls_names[int(preds[indeks])]} else \"red\"))\n",
    "        else: plt.title(f\"GT -> {gt} ; PRED -> {pred}\")\n",
    "\n",
    "m.load_state_dict(torch.load(f\"{save_dir}/{save_prefix}_best_model.pth\"))\n",
    "m.eval()\n",
    "final_conv, fc_params = m.features[-1], list(m.head.fc.parameters())\n",
    "inference(model = m.to(device), device = device, test_dl = ts_dl, num_ims = 20, row = 4, cls_names = list(classes.keys()), final_conv = final_conv, fc_params = fc_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c566b2-992d-42c6-b36d-3dd8e0f3f127",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
